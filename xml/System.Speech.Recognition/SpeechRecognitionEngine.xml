<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <Metadata>
    <Meta Name="ms.openlocfilehash" Value="0550a278074dc65a5b57616fb3bdf64a8ce83092" />
    <Meta Name="ms.sourcegitcommit" Value="5a49536d99d2d0b54e4cb7280870903e043272df" />
    <Meta Name="ms.translationtype" Value="HT" />
    <Meta Name="ms.contentlocale" Value="es-ES" />
    <Meta Name="ms.lasthandoff" Value="07/03/2018" />
    <Meta Name="ms.locfileid" Value="37756679" />
  </Metadata>
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <TypeSignature Language="VB.NET" Value="Public Class SpeechRecognitionEngine&#xA;Implements IDisposable" />
  <TypeSignature Language="C++ CLI" Value="public ref class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="F#" Value="type SpeechRecognitionEngine = class&#xA;    interface IDisposable" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>3.0.0.0</AssemblyVersion>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Proporciona los medios para obtener acceso y administrar un motor de reconocimiento de voz en curso.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Puede crear una instancia de esta clase para cualquiera de los reconocedores de voz instalado. Para obtener información acerca de los reconocedores están instalados, use estático <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Esta clase es para la ejecución de speech recognition motores en proceso y proporciona control sobre diversos aspectos del reconocimiento de voz, como sigue:  
  
-   Para crear un módulo de reconocimiento de voz en curso, use uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> constructores.  
  
-   Para administrar las gramáticas reconocimiento de voz, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> métodos y el <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propiedad.  
  
-   Para configurar la entrada para el reconocimiento, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> método.  
  
-   Para llevar a cabo el reconocimiento de voz, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método.  
  
-   Para modificar cómo controla el reconocimiento de silencio o entrada inesperada, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
-   Para cambiar el número de las alternativas que devuelve el reconocedor, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> propiedad. El reconocedor devuelve los resultados del reconocimiento en un <xref:System.Speech.Recognition.RecognitionResult> objeto.  
  
-   Para sincronizar los cambios realizados en el módulo de reconocimiento, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método. El reconocedor utiliza más de un subproceso para realizar tareas.  
  
-   Para emular la entrada para el reconocimiento, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 La <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto es para uso único del proceso que crea una instancia del objeto. Por el contrario, el <xref:System.Speech.Recognition.SpeechRecognizer> comparte un único módulo de reconocimiento con cualquier aplicación que quiera usarla.  
  
> [!NOTE]
>  Llame siempre a <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> antes de liberar la última referencia para el reconocimiento de voz. En caso contrario, los recursos que utiliza no se liberarán hasta que el recolector de elementos no utilizados llama al objeto reconocedor `Finalize` método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. Dado que este ejemplo se usa el `Multiple` el modo de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método, realiza el reconocimiento hasta que cierre la ventana de consola o detener la depuración.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
    <altmember cref="T:System.Speech.Recognition.Grammar" />
    <altmember cref="T:System.Speech.Recognition.SpeechRecognizer" />
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicializa una nueva instancia de la clase <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Puede construir un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia desde cualquiera de las siguientes acciones:  
  
-   El motor de reconocimiento de voz predeterminado para el sistema  
  
-   Un motor de reconocimiento de voz específico que se especifica por nombre  
  
-   El motor de reconocimiento de voz predeterminado para una configuración regional que especifique  
  
-   Un motor de reconocimiento específicos que cumple los criterios que especifique en un <xref:System.Speech.Recognition.RecognizerInfo> objeto.  
  
 Antes de que el reconocedor de voz puede comenzar reconocimiento, debe cargar la gramática de reconocimiento de voz al menos una y configurar la entrada para el reconocedor.  
  
 Para cargar una gramática, llame a la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberSignature Language="VB.NET" Value="Public Sub New ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine();" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicializa una nueva instancia de la clase <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizando el reconocedor de voz predeterminado para el sistema.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Antes de iniciar el reconocimiento de voz del reconocedor de voz, debe cargar al menos una gramática de reconocimiento y configurar la entrada para el reconocedor.  
  
 Para cargar una gramática, llame a la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (culture As CultureInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Globalization::CultureInfo ^ culture);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Globalization.CultureInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine culture" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">Configuración regional que el módulo de reconocimiento de voz debe admitir.</param>
        <summary>Inicializa una nueva instancia de la clase <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizando el reconocedor de voz predeterminado para una configuración regional especificada.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows y la API System.Speech aceptan todos los códigos de idioma de país válidos. Para realizar el reconocimiento de voz con el idioma especificado en el `CultureInfo` argumento, un motor de reconocimiento de voz que admite que se debe instalar el código de país de lenguaje. Los motores de reconocimiento de voz que se incluye con Microsoft Windows 7 funcionan con los siguientes códigos de país de lenguaje.  
  
-   en-GB. Inglés (Reino Unido)  
  
-   en-US. Inglés (Estados Unidos)  
  
-   de-DE. Alemán (Alemania)  
  
-   es-es al directorio. Español (España)  
  
-   fr-FR. Francés (Francia)  
  
-   ja-JP. Japonés (Japón)  
  
-   zh-CN. Chino (China)  
  
-   zh-TW. Chino (Taiwán)  
  
 Códigos de idioma de dos letras como "es-es", "fr", o "es" también se permiten.  
  
 Antes de que el reconocedor de voz puede comenzar reconocimiento, debe cargar la gramática de reconocimiento de voz al menos una y configurar la entrada para el reconocedor.  
  
 Para cargar una gramática, llame a la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica e inicializa un reconocedor de voz para la configuración regional en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Ninguno de los reconocedores de voz instalados admite la configuración regional especificada, o <paramref name="culture" /> es la referencia cultural invariable.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> es <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::Speech::Recognition::RecognizerInfo ^ recognizerInfo);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : System.Speech.Recognition.RecognizerInfo -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerInfo" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">La información del módulo de reconocimiento de voz concreto.</param>
        <summary>Inicializa una nueva instancia de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> utilizando la información de un objeto <see cref="T:System.Speech.Recognition.RecognizerInfo" /> para especificar el reconocedor que de va a utilizar.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Puede crear una instancia de esta clase para cualquiera de los reconocedores de voz instalado. Para obtener información acerca de los reconocedores están instalados, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Antes de que el reconocedor de voz puede comenzar reconocimiento, debe cargar la gramática de reconocimiento de voz al menos una y configurar la entrada para el reconocedor.  
  
 Para cargar una gramática, llame a la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica e inicializa un reconocedor de voz que admite el idioma inglés.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub New (recognizerId As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; SpeechRecognitionEngine(System::String ^ recognizerId);" />
      <MemberSignature Language="F#" Value="new System.Speech.Recognition.SpeechRecognitionEngine : string -&gt; System.Speech.Recognition.SpeechRecognitionEngine" Usage="new System.Speech.Recognition.SpeechRecognitionEngine recognizerId" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">El nombre de token de módulo de reconocimiento de voz para usar.</param>
        <summary>Inicializa una nueva instancia de la clase <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con un parámetro de cadena que especifica el nombre del reconocedor que se va a utilizar.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El nombre del token del reconocedor es el valor de la <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> propiedad de la <xref:System.Speech.Recognition.RecognizerInfo> objeto devuelto por la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propiedad del reconocedor. Para obtener una colección de todos los reconocedores instalados, use estático <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Antes de que el reconocedor de voz puede comenzar reconocimiento, debe cargar la gramática de reconocimiento de voz al menos una y configurar la entrada para el reconocedor.  
  
 Para cargar una gramática, llame a la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básico y crea una instancia de la 8.0 del reconocedor de voz de Windows (inglés - Estados Unidos).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">No se instala ningún reconocedor de voz con ese nombre de token o <paramref name="recognizerId" /> es una cadena vacía ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> es <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioFormat As SpeechAudioFormatInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::AudioFormat::SpeechAudioFormatInfo ^ AudioFormat { System::Speech::AudioFormat::SpeechAudioFormatInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioFormat : System.Speech.AudioFormat.SpeechAudioFormatInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene el formato del sonido que es recibido por <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>El formato de audio en la entrada para la instancia de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> o <see langword="null" /> si la entrada no se ha configurado ni establecido en la entrada NULL.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para configurar la entrada de audio, use uno de los métodos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 El ejemplo siguiente usa <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> para obtener y mostrar datos de formato de audio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.AudioFormat.SpeechAudioFormatInfo" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioFormat" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioLevel As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int AudioLevel { int get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioLevel : int" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene el nivel del sonido que es recibido por <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>El nivel de audio de la entrada al módulo de reconocimiento de voz, de 0 a 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El valor 0 representa silencio y 100 representa el volumen de entrada máximo.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioLevelUpdated As EventHandler(Of AudioLevelUpdatedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioLevelUpdatedEventArgs ^&gt; ^ AudioLevelUpdated;" />
      <MemberSignature Language="F#" Value="member this.AudioLevelUpdated : EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " Usage="member this.AudioLevelUpdated : System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> indica el nivel de su entrada de audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera este evento varias veces por segundo. Depende de la frecuencia con la que se genera el evento en el equipo donde se ejecuta la aplicación.  
  
 Para obtener el nivel de audio en el momento del evento, use la <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> propiedad del asociado <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Para obtener el nivel de audio actual de la entrada al reconocedor, use el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> propiedad.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 En el ejemplo siguiente se agrega un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> eventos a un <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto. El controlador genera el nuevo nivel de audio en la consola.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioLevelUpdatedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan AudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene la ubicación actual en la secuencia audio generada por el dispositivo que está proporcionando entradas al módulo de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>La ubicación actual en la secuencia de audio generada por el dispositivo de entrada.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propiedad hace referencia a la posición del dispositivo de entrada en su secuencia de audio generado. Por el contrario, el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propiedad hace referencia a la posición del reconocedor dentro de su entrada de audio. Estas posiciones pueden ser diferentes. Por ejemplo, si el reconocedor ha recibido de entrada para que TI no tiene aún genera un resultado de reconocimiento, a continuación, el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propiedad es menor que el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propiedad.  
  
   
  
## Examples  
 En el ejemplo siguiente, el reconocimiento de voz en el proceso utiliza una gramática de dictado para que coincida con la entrada de voz. Un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> escribe eventos en la consola la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> cuando el reconocedor de voz detecta la voz en su entrada.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioSignalProblemOccurred As EventHandler(Of AudioSignalProblemOccurredEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioSignalProblemOccurredEventArgs ^&gt; ^ AudioSignalProblemOccurred;" />
      <MemberSignature Language="F#" Value="member this.AudioSignalProblemOccurred : EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " Usage="member this.AudioSignalProblemOccurred : System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> detecta un problema en la señal de audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obtener el problema, use el <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> propiedad del asociado <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 En el ejemplo siguiente se define un controlador de eventos que recopila información sobre un <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> eventos.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblem" />
        <altmember cref="T:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property AudioState As AudioState" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::AudioState AudioState { System::Speech::Recognition::AudioState get(); };" />
      <MemberSignature Language="F#" Value="member this.AudioState : System.Speech.Recognition.AudioState" Usage="System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene el estado del sonido que es recibido por <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>El estado de la entrada de audio en el módulo de reconocimiento de voz.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propiedad representa el estado de audio con un miembro de la <xref:System.Speech.Recognition.AudioState> enumeración.  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognizer.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event AudioStateChanged As EventHandler(Of AudioStateChangedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::AudioStateChangedEventArgs ^&gt; ^ AudioStateChanged;" />
      <MemberSignature Language="F#" Value="member this.AudioStateChanged : EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " Usage="member this.AudioStateChanged : System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando cambia el estado en el sonido que recibe el <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obtener el estado de audio en el momento del evento, use la <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> propiedad del asociado <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Para obtener el estado de la entrada para el reconocimiento de audio actual, use el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propiedad. Para obtener más información sobre el estado de audio, consulte el <xref:System.Speech.Recognition.AudioState> enumeración.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 En el ejemplo siguiente se usa un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> del nuevo evento para el reconocedor de escritura <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> en la consola cada vez que los cambios, con un miembro de la <xref:System.Speech.Recognition.AudioState> enumeración.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.AudioState" />
        <altmember cref="T:System.Speech.Recognition.AudioStateChangedEventArgs" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property BabbleTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan BabbleTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.BabbleTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene o establece el intervalo de tiempo durante el que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> acepta la entrada que contiene solo ruido de fondo, antes de finalizar el reconocimiento.</summary>
        <value>La duración del intervalo de tiempo.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada módulo de reconocimiento de voz tiene un algoritmo para distinguir entre latencia y voz. El reconocedor se clasifica como ruido de fondo no-silencio de entrada que no coincide con la regla inicial de cualquiera de reconocimiento de la carga y habilita gramáticas reconocimiento de voz. Si el reconocedor recibe solo ruido de fondo y silencio dentro del intervalo de tiempo de espera de habla, el reconocedor finaliza esa operación de reconocimiento.  
  
-   Para las operaciones de reconocimiento asincrónica, se genera el reconocedor la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, donde el <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> propiedad es `true`y el <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> propiedad es `null`.  
  
-   Para las operaciones de reconocimiento sincrónica y emulación, devuelve el reconocedor `null`, en lugar de válido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Si el período de tiempo de espera de habla se establece en 0, el reconocedor no lleva a cabo una comprobación de tiempo de espera de habla. El intervalo de tiempo de espera puede ser cualquier valor no negativo. El valor predeterminado es 0 segundos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básico que establece el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> las propiedades de un <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de iniciar el reconocimiento de voz. Controladores para el reconocimiento de voz <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos de salida de información de evento en la consola para demostrar cómo el <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> las propiedades de un <xref:System.Speech.Recognition.SpeechRecognitionEngine> afectan a las operaciones de reconocimiento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propiedad se establece en un valor menor que 0 segundos.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Dispose">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Desecha el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
      </Docs>
    </MemberGroup>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberSignature Language="VB.NET" Value="Public Sub Dispose ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; virtual void Dispose();" />
      <MemberSignature Language="F#" Value="abstract member Dispose : unit -&gt; unit&#xA;override this.Dispose : unit -&gt; unit" Usage="speechRecognitionEngine.Dispose " />
      <MemberType>Method</MemberType>
      <Implements>
        <InterfaceMember>M:System.IDisposable.Dispose</InterfaceMember>
      </Implements>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Desecha el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberSignature Language="VB.NET" Value="Protected Overridable Sub Dispose (disposing As Boolean)" />
      <MemberSignature Language="C++ CLI" Value="protected:&#xA; virtual void Dispose(bool disposing);" />
      <MemberSignature Language="F#" Value="abstract member Dispose : bool -&gt; unit&#xA;override this.Dispose : bool -&gt; unit" Usage="speechRecognitionEngine.Dispose disposing" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">Es <see langword="true" /> para liberar tanto recursos administrados como no administrados; es <see langword="false" /> para liberar únicamente recursos no administrados.</param>
        <summary>Desecha el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> y libera los recursos utilizados durante la sesión.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula la entrada para el reconocimiento de voz, usando el texto en lugar de audio para el reconocimiento de voz sincrónico.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Estos métodos omitir la entrada de audio del sistema y proporcionar el texto para el reconocimiento como <xref:System.String> objetos o como una matriz de <xref:System.Speech.Recognition.RecognizedWordUnit> objetos. Esto puede resultar útil cuando se está probando o depuración de una aplicación o gramática. Por ejemplo, puede utilizar la emulación para determinar si es una palabra en una gramática y qué semántica se devuelve cuando se reconoce la palabra. Use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> método para deshabilitar la entrada de audio para el motor de reconocimiento de voz durante las operaciones de emulación.  
  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula. El reconocedor omite las líneas nuevas y el espacio en blanco adicional y trata los signos de puntuación como entrada literal.  
  
> [!NOTE]
>  El <xref:System.Speech.Recognition.RecognitionResult> objeto generado por el reconocedor de voz en respuesta a entrada emulada tiene un valor de `null` para su <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propiedad.  
  
 Para emular el reconocimiento asincrónico, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> método.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function EmulateRecognize (inputText As String) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">La entrada de la operación de reconocimiento.</param>
        <summary>Emula la entrada de una frase al módulo de reconocimiento de voz, usando el texto en lugar de audio para el reconocimiento de voz sincrónico.</summary>
        <returns>El resultado de la operación de reconocimiento, o <see langword="null" /> si la operación no es correcta o el reconocedor no está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula.  
  
 Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas y ancho de caracteres al aplicar reglas de gramática a la frase de entrada. Para obtener más información acerca de este tipo de comparación, consulte el <xref:System.Globalization.CompareOptions> valores de enumeración <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> y <xref:System.Globalization.CompareOptions.IgnoreWidth>. Los reconocedores también omite las líneas nuevas y espacio en blanco adicional y tratan los signos de puntuación como entrada literal.  
  
   
  
## Examples  
 El siguiente ejemplo de código forma parte de una aplicación de consola que muestra los eventos asociados generados por el reconocedor de voz, los resultados del reconocimiento asociado y entrada emulada. El ejemplo genera el siguiente resultado.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> es la cadena vacía ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matriz de unidades de palabra que contiene la entrada de la operación de reconocimiento.</param>
        <param name="compareOptions">Combinación bit a bit de los valores de enumeración que describen el tipo de comparación que se va a usar para la operación emulada de reconocimiento.</param>
        <summary>Emula la entrada de palabras específicas al módulo de reconocimiento de voz, usando texto en lugar de audio para el reconocimiento de voz sincrónico, y especifica cómo el reconocedor controla la comparación Unicode entre las palabras y las gramáticas reconocimiento de voz cargadas.</summary>
        <returns>El resultado de la operación de reconocimiento, o <see langword="null" /> si la operación no es correcta o el reconocedor no está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula.  
  
 El reconocedor utiliza `compareOptions` cuando se aplica reglas de gramática a la frase de entrada. Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas si la <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. El reconocedor siempre omite el ancho de caracteres y nunca omite el tipo Kana. El reconocedor también omite las líneas nuevas y el espacio en blanco adicional y trata los signos de puntuación como entrada literal. Para obtener más información sobre el ancho de caracteres y el tipo de Kana, consulte el <xref:System.Globalization.CompareOptions> enumeración.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o varios elementos <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene la marca <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ EmulateRecognize(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognize : string * System.Globalization.CompareOptions -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.EmulateRecognize (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">La frase de entrada de la operación de reconocimiento.</param>
        <param name="compareOptions">Combinación bit a bit de los valores de enumeración que describen el tipo de comparación que se va a usar para la operación emulada de reconocimiento.</param>
        <summary>Emula la entrada de una frase al módulo de reconocimiento de voz, usando texto en lugar de audio para el reconocimiento de voz sincrónico, y especifica cómo el reconocedor controla la comparación Unicode entre la frase y las gramáticas reconocimiento de voz cargadas.</summary>
        <returns>El resultado de la operación de reconocimiento, o <see langword="null" /> si la operación no es correcta o el reconocedor no está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula.  
  
 El reconocedor utiliza `compareOptions` cuando se aplica reglas de gramática a la frase de entrada. Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas si la <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. El reconocedor siempre omite el ancho de caracteres y nunca omite el tipo Kana. El reconocedor también omite las líneas nuevas y el espacio en blanco adicional y trata los signos de puntuación como entrada literal. Para obtener más información sobre el ancho de caracteres y el tipo de Kana, consulte el <xref:System.Globalization.CompareOptions> enumeración.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> es la cadena vacía ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene la marca <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula la entrada para el reconocimiento de voz, usando el texto en lugar de audio para el reconocimiento de voz asincrónico.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Estos métodos omitir la entrada de audio del sistema y proporcionar el texto para el reconocimiento como <xref:System.String> objetos o como una matriz de <xref:System.Speech.Recognition.RecognizedWordUnit> objetos. Esto puede resultar útil cuando se está probando o depuración de una aplicación o gramática. Por ejemplo, puede utilizar la emulación para determinar si es una palabra en una gramática y qué semántica se devuelve cuando se reconoce la palabra. Use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> método para deshabilitar la entrada de audio para el motor de reconocimiento de voz durante las operaciones de emulación.  
  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula. Cuando el reconocedor finaliza la operación de reconocimiento asincrónica, se genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos. El reconocedor omite las líneas nuevas y el espacio en blanco adicional y trata los signos de puntuación como entrada literal.  
  
> [!NOTE]
>  El <xref:System.Speech.Recognition.RecognitionResult> objeto generado por el reconocedor de voz en respuesta a entrada emulada tiene un valor de `null` para su <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propiedad.  
  
 Para emular el reconocimiento de sincrónico, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> método.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub EmulateRecognizeAsync (inputText As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync inputText" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">La entrada de la operación de reconocimiento.</param>
        <summary>Emula la entrada de una frase al módulo de reconocimiento de voz, usando el texto en lugar de audio para el reconocimiento de voz asincrónico.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula. Cuando el reconocedor finaliza la operación de reconocimiento asincrónica, se genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos.  
  
 Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas y ancho de caracteres al aplicar reglas de gramática a la frase de entrada. Para obtener más información acerca de este tipo de comparación, consulte el <xref:System.Globalization.CompareOptions> valores de enumeración <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> y <xref:System.Globalization.CompareOptions.IgnoreWidth>. Los reconocedores también omite las líneas nuevas y espacio en blanco adicional y tratan los signos de puntuación como entrada literal.  
  
   
  
## Examples  
 El siguiente ejemplo de código forma parte de una aplicación de consola que muestra la entrada emulada asincrónica, los resultados de reconocimiento asociados y los eventos asociados generados por el reconocedor de voz. El ejemplo genera el siguiente resultado.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada, o el reconocedor tiene una operación asincrónica de reconocimiento que aún no se ha completado.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> es la cadena vacía ("").</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(cli::array &lt;System::Speech::Recognition::RecognizedWordUnit ^&gt; ^ wordUnits, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : System.Speech.Recognition.RecognizedWordUnit[] * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (wordUnits, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Matriz de unidades de palabra que contiene la entrada de la operación de reconocimiento.</param>
        <param name="compareOptions">Combinación bit a bit de los valores de enumeración que describen el tipo de comparación que se va a usar para la operación emulada de reconocimiento.</param>
        <summary>Emula la entrada de palabras específicas al módulo de reconocimiento de voz, usando una matriz de objetos <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> en lugar de audio para el reconocimiento de voz asincrónico, y especifica cómo el reconocedor controla la comparación Unicode entre las palabras y las gramáticas reconocimiento de voz cargadas.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula. Cuando el reconocedor finaliza la operación de reconocimiento asincrónica, se genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos.  
  
 El reconocedor utiliza `compareOptions` cuando se aplica reglas de gramática a la frase de entrada. Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas si la <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. Los reconocedores de ignorar siempre el ancho de caracteres y nunca omitir el tipo Kana. Los reconocedores también omite las líneas nuevas y espacio en blanco adicional y tratan los signos de puntuación como entrada literal. Para obtener más información sobre el ancho de caracteres y el tipo de Kana, consulte el <xref:System.Globalization.CompareOptions> enumeración.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada, o el reconocedor tiene una operación asincrónica de reconocimiento que aún no se ha completado.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" /> contiene uno o varios elementos <see langword="null" />.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene la marca <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void EmulateRecognizeAsync(System::String ^ inputText, System::Globalization::CompareOptions compareOptions);" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeAsync : string * System.Globalization.CompareOptions -&gt; unit" Usage="speechRecognitionEngine.EmulateRecognizeAsync (inputText, compareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">La frase de entrada de la operación de reconocimiento.</param>
        <param name="compareOptions">Combinación bit a bit de los valores de enumeración que describen el tipo de comparación que se va a usar para la operación emulada de reconocimiento.</param>
        <summary>Emula la entrada de una frase al módulo de reconocimiento de voz, usando texto en lugar de audio para el reconocimiento de voz asincrónico, y especifica cómo el reconocedor controla la comparación Unicode entre la frase y las gramáticas reconocimiento de voz cargadas.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La genera reconocedor de voz el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como si la operación de reconocimiento no se emula. Cuando el reconocedor finaliza la operación de reconocimiento asincrónica, se genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos.  
  
 El reconocedor utiliza `compareOptions` cuando se aplica reglas de gramática a la frase de entrada. Los identificadores de las que se incluyen en la Vista y Windows 7 Omitir mayúsculas y minúsculas si la <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> o <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. Los reconocedores de ignorar siempre el ancho de caracteres y nunca omitir el tipo Kana. Los reconocedores también omite las líneas nuevas y espacio en blanco adicional y tratan los signos de puntuación como entrada literal. Para obtener más información sobre el ancho de caracteres y el tipo de Kana, consulte el <xref:System.Globalization.CompareOptions> enumeración.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">El reconocedor no tiene ninguna gramática de reconocimiento de voz cargada, o el reconocedor tiene una operación asincrónica de reconocimiento que aún no se ha completado.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> es la cadena vacía ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" /> contiene la marca <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" /> o <see cref="F:System.Globalization.CompareOptions.StringSort" />.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event EmulateRecognizeCompleted As EventHandler(Of EmulateRecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::EmulateRecognizeCompletedEventArgs ^&gt; ^ EmulateRecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.EmulateRecognizeCompleted : EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " Usage="member this.EmulateRecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> concluye una operación de reconocimiento asincrónica de entrada emulada.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> método comienza una operación asincrónica de reconocimiento. El <xref:System.Speech.Recognition.SpeechRecognitionEngine> provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento cuando finaliza la operación asincrónica.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operación puede generar el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. El <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos son el último evento que provoca el reconocedor para una operación determinada.  
  
 Si el reconocimiento emulado fue correcto, puede acceder al resultado del reconocimiento mediante cualquiera de las siguientes acciones:  
  
-   El <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> propiedad en el <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> objeto en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propiedad en el <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> objeto en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos.  
  
 Si no fue correcto, el reconocimiento emulado el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> no se produce el evento y el <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> será null.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> se deriva de <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> se deriva de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente es parte de una aplicación de consola que carga una gramática de reconocimiento de voz y muestra la entrada emulada asincrónica, los resultados de reconocimiento asociados y los eventos asociados generados por el reconocedor de voz.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene o establece el intervalo de silencio que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceptará al final de la entrada no ambigua antes de finalizar una operación de reconocimiento.</summary>
        <value>La duración del intervalo de silencio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor de voz usa este intervalo de tiempo de espera cuando la entrada de reconocimiento es ambigua. Por ejemplo, para una gramática de reconocimiento de voz que admite el reconocimiento del "nuevo de juegos," o "nueva partida", "nueva de juegos," es una entrada no ambigua, y "nuevo juego" es una entrada ambigua.  
  
 Esta propiedad determina cuánto tiempo espera el motor de reconocimiento de voz para una entrada adicional antes de finalizar una operación de reconocimiento. El intervalo de tiempo de espera puede ser de 0 segundos a 10 segundos, ambos inclusivas. El valor predeterminado es 150 milisegundos.  
  
 Para establecer el intervalo de tiempo de espera de entrada ambigua, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedad.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propiedad se establece en un valor menor que 0 segundos o mayor que 10 segundos.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="VB.NET" Value="Public Property EndSilenceTimeoutAmbiguous As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan EndSilenceTimeoutAmbiguous { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.EndSilenceTimeoutAmbiguous : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene o establece el intervalo de silencio que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceptará al final de la entrada ambigua antes de finalizar una operación de reconocimiento.</summary>
        <value>La duración del intervalo de silencio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor de voz usa este intervalo de tiempo de espera cuando la entrada de reconocimiento es ambigua. Por ejemplo, para una gramática de reconocimiento de voz que admite el reconocimiento del "nuevo de juegos," o "nueva partida", "nueva de juegos," es una entrada no ambigua, y "nuevo juego" es una entrada ambigua.  
  
 Esta propiedad determina cuánto tiempo espera el motor de reconocimiento de voz para una entrada adicional antes de finalizar una operación de reconocimiento. El intervalo de tiempo de espera puede ser de 0 segundos a 10 segundos, ambos inclusivas. El valor predeterminado es 500 milisegundos.  
  
 Para establecer el intervalo de tiempo de espera de entrada no ambigua, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propiedad.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propiedad se establece en un valor menor que 0 segundos o mayor que 10 segundos.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property Grammars As ReadOnlyCollection(Of Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ Grammars { System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::Grammar ^&gt; ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.Grammars : System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene una colección de los objetos <see cref="T:System.Speech.Recognition.Grammar" /> cargados en esta instancia <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>La colección de objetos <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 El siguiente ejemplo envía información a la consola para cada gramática de reconocimiento de voz cargada actualmente por un reconocedor de voz.  
  
> [!IMPORTANT]
>  Copiar la colección de gramática para evitar errores si se modifica la colección mientras este método enumera los elementos de la colección.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.Grammar" />
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberSignature Language="VB.NET" Value="Public Property InitialSilenceTimeout As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan InitialSilenceTimeout { TimeSpan get(); void set(TimeSpan value); };" />
      <MemberSignature Language="F#" Value="member this.InitialSilenceTimeout : TimeSpan with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene o establece el intervalo de tiempo durante el que <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> acepta la entrada que contiene solo silencio, antes de finalizar el reconocimiento.</summary>
        <value>La duración del intervalo de silencio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada módulo de reconocimiento de voz tiene un algoritmo para distinguir entre latencia y voz. Si la entrada del reconocedor silencio durante el período de tiempo de espera de silencio inicial, el reconocedor finaliza esa operación de reconocimiento.  
  
-   Para las operaciones de reconocimiento asincrónica y emulación, genera el reconocedor la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, donde el <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> propiedad es `true`y el <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> propiedad es `null`.  
  
-   Para las operaciones de reconocimiento sincrónica y emulación, devuelve el reconocedor `null`, en lugar de válido <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Si el intervalo de tiempo de espera de silencio inicial se establece en 0, el reconocedor no lleva a cabo una comprobación de tiempo de espera de silencio inicial. El intervalo de tiempo de espera puede ser cualquier valor no negativo. El valor predeterminado es 0 segundos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo se establece la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> las propiedades de un <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de iniciar el reconocimiento de voz. Controladores para el reconocimiento de voz <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos de salida de información de evento en la consola para demostrar cómo el <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> las propiedades de un <xref:System.Speech.Recognition.SpeechRecognitionEngine> propiedades afectan a las operaciones de reconocimiento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propiedad se establece en un valor menor que 0 segundos.</exception>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberSignature Language="VB.NET" Value="Public Shared Function InstalledRecognizers () As ReadOnlyCollection(Of RecognizerInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; static System::Collections::ObjectModel::ReadOnlyCollection&lt;System::Speech::Recognition::RecognizerInfo ^&gt; ^ InstalledRecognizers();" />
      <MemberSignature Language="F#" Value="static member InstalledRecognizers : unit -&gt; System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;" Usage="System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Devuelve información para todos los reconocedores de voz instalados en el sistema actual.</summary>
        <returns>Una colección de solo lectura de los objetos <see cref="T:System.Speech.Recognition.RecognizerInfo" /> que describen los reconocedores instalados.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obtener información sobre el reconocedor actual, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propiedad.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo utiliza la colección devuelta por la <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método para buscar un reconocedor de voz que admite el idioma inglés.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">El objeto de gramática para cargar.</param>
        <summary>De forma sincrónica carga un objeto <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor produce una excepción si el <xref:System.Speech.Recognition.Grammar> objeto ya está cargado, se cargan de forma asincrónica o no ha podido cargar en cualquier módulo de reconocimiento. No se puede cargar el mismo <xref:System.Speech.Recognition.Grammar> objeto en varias instancias de <xref:System.Speech.Recognition.SpeechRecognitionEngine>. En su lugar, cree un nuevo <xref:System.Speech.Recognition.Grammar> para cada objeto <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia.  
  
 Si se está ejecutando el reconocedor, las aplicaciones deben usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar el motor de reconocimiento de voz antes de cargar, descargar, habilitar o deshabilitar una gramática.  
  
 Cuando se carga una gramática, está habilitado de forma predeterminada. Para deshabilitar una gramática cargada, use el <xref:System.Speech.Recognition.Grammar.Enabled%2A> propiedad.  
  
 Para cargar un <xref:System.Speech.Recognition.Grammar> objetos de forma asincrónica, use la <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo se crea un <xref:System.Speech.Recognition.DictationGrammar> y los carga en un reconocedor de voz.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> es <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> no está en un estado válido.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void LoadGrammarAsync(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarAsync : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.LoadGrammarAsync grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">La gramática de reconocimiento de voz para cargar.</param>
        <summary>De forma asincrónica carga una gramática de reconocimiento de voz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cuando el reconocedor finaliza la carga un <xref:System.Speech.Recognition.Grammar> objeto, genera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> eventos. El reconocedor produce una excepción si el <xref:System.Speech.Recognition.Grammar> objeto ya está cargado, se cargan de forma asincrónica o no ha podido cargar en cualquier módulo de reconocimiento. No se puede cargar el mismo <xref:System.Speech.Recognition.Grammar> objeto en varias instancias de <xref:System.Speech.Recognition.SpeechRecognitionEngine>. En su lugar, cree un nuevo <xref:System.Speech.Recognition.Grammar> para cada objeto <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia.  
  
 Si se está ejecutando el reconocedor, las aplicaciones deben usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar el motor de reconocimiento de voz antes de cargar, descargar, habilitar o deshabilitar una gramática.  
  
 Cuando se carga una gramática, está habilitado de forma predeterminada. Para deshabilitar una gramática cargada, use el <xref:System.Speech.Recognition.Grammar.Enabled%2A> propiedad.  
  
 Para cargar de forma sincrónica una gramática de reconocimiento de voz, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> método.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> es <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> no está en un estado válido.</exception>
        <exception cref="T:System.OperationCanceledException">La operación asincrónica se canceló.</exception>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event LoadGrammarCompleted As EventHandler(Of LoadGrammarCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::LoadGrammarCompletedEventArgs ^&gt; ^ LoadGrammarCompleted;" />
      <MemberSignature Language="F#" Value="member this.LoadGrammarCompleted : EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " Usage="member this.LoadGrammarCompleted : System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> finaliza la carga asincrónica de un objeto <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método inicia una operación asincrónica. El <xref:System.Speech.Recognition.SpeechRecognitionEngine> provoca este evento cuando se complete la operación. Para obtener el <xref:System.Speech.Recognition.Grammar> que carga el módulo de reconocimiento de objetos, utilice el <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> propiedad del asociado <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Para obtener actual <xref:System.Speech.Recognition.Grammar> objetos que se haya cargado el reconocedor, usar el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propiedad.  
  
 Si se está ejecutando el reconocedor, las aplicaciones deben usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar el motor de reconocimiento de voz antes de cargar, descargar, habilitar o deshabilitar una gramática.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente crea un módulo de reconocimiento de voz en curso y, a continuación, crea dos tipos de gramáticas para reconocer palabras específicas y para la aceptación dictados libres. El ejemplo se crea un <xref:System.Speech.Recognition.Grammar> objeto de cada una de las gramáticas de reconocimiento de voz completa, a continuación, carga de forma asincrónica el <xref:System.Speech.Recognition.Grammar> objetos a la <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia. Controladores para el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos escriben en la consola el nombre de la <xref:System.Speech.Recognition.Grammar> objeto que se usó para realizar el reconocimiento y el texto del resultado de reconocimiento, respectivamente.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.LoadGrammarCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberSignature Language="VB.NET" Value="Public Property MaxAlternates As Integer" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property int MaxAlternates { int get(); void set(int value); };" />
      <MemberSignature Language="F#" Value="member this.MaxAlternates : int with get, set" Usage="System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene o establece el número máximo de resultados alternativos de reconocimiento que el <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> devuelve para cada operación de reconocimiento.</summary>
        <value>El número de resultados alternativos para devolver.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> propiedad de la <xref:System.Speech.Recognition.RecognitionResult> clase contiene la colección de <xref:System.Speech.Recognition.RecognizedPhrase> objetos que representan posibles interpretaciones de la entrada.  
  
 El valor predeterminado de <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> es 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Se ha establecido <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" /> en un valor menor que 0.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Function QueryRecognizerSetting (settingName As String) As Object" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Object ^ QueryRecognizerSetting(System::String ^ settingName);" />
      <MemberSignature Language="F#" Value="member this.QueryRecognizerSetting : string -&gt; obj" Usage="speechRecognitionEngine.QueryRecognizerSetting settingName" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nombre de la configuración que se devolverá.</param>
        <summary>Devuelve los valores de configuración para el reconocedor.</summary>
        <returns>El valor de la configuración.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configuración del módulo de reconocimiento puede contener la cadena, entero de 64 bits o datos de la dirección de memoria. En la tabla siguiente se describe la configuración que se define para una API de Microsoft Speech (SAPI)-reconocedor compatible. La siguiente configuración debe tener el mismo intervalo para cada módulo de reconocimiento que admita la configuración. Un reconocedor conforme SAPI no es necesario para admitir esta configuración y puede admitir otras opciones.  
  
|nombre|Descripción|  
|----------|-----------------|  
|`ResourceUsage`|Especifica el consumo de CPU del reconocedor. El intervalo es de 0 a 100. El valor predeterminado es 50.|  
|`ResponseSpeed`|Indica la longitud de silencio al final de la entrada no ambigua antes de que el reconocedor de voz completa una operación de reconocimiento. El intervalo es de 0 a 10 000 milisegundos (ms). Esta opción corresponde al reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propiedad.  Valor predeterminado = 150 ms.|  
|`ComplexResponseSpeed`|Indica la longitud de silencio al final de la entrada ambigua antes de que el reconocedor de voz completa una operación de reconocimiento. El intervalo es de 0 a 10 000 ms. Esta opción corresponde al reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedad. Valor predeterminado = 500 ms.|  
|`AdaptationOn`|Indica si la adaptación del modelo acústico es ON (valor = `1`) u OFF (valor = `0`). El valor predeterminado es `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica si la adaptación de fondo es ON (valor = `1`) u OFF (valor = `0`), y se conserva la configuración en el registro. El valor predeterminado es `1` (ON).|  
  
 Para actualizar una configuración para el reconocedor, use uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos.  
  
   
  
## Examples  
 El ejemplo siguiente es parte de una aplicación de consola que genera los valores de un número de la configuración definida para el reconocedor que admita la configuración regional en-US. El ejemplo genera el siguiente resultado.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> es la cadena vacía ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">El reconocedor no tiene un valor con ese nombre.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicia una operación de reconocimiento de voz sincrónica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Estos métodos realizan una operación de reconocimiento único, sincrónico. El reconocedor realiza esta operación en sus gramáticas de reconocimiento de voz cargados y habilitados.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
 El reconocedor no provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos al utilizar uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos devuelven un <xref:System.Speech.Recognition.RecognitionResult> objeto, o `null` si la operación no es correcta o el reconocedor no está habilitado.  
  
 Una operación de reconocimiento sincrónica puede producir errores por las razones siguientes:  
  
-   No se detecta la voz antes de que caduquen los intervalos de tiempo de espera para la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propiedades, o para el `initialSilenceTimeout` parámetro de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> método.  
  
-   El motor de reconocimiento detecta la voz, pero no encuentra ninguna coincidencia en cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para modificar cómo el reconocedor controla la temporización de voz o silencio con respecto al reconocimiento, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine> debe tener al menos un <xref:System.Speech.Recognition.Grammar> objeto cargado antes de realizar el reconocimiento. Para cargar una gramática de reconocimiento de voz, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para llevar a cabo el reconocimiento asincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize () As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize();" />
      <MemberSignature Language="F#" Value="member this.Recognize : unit -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Realiza una operación de reconocimiento de voz sincrónica.</summary>
        <returns>El resultado de reconocimiento para la entrada, o <see langword="null" /> si la operación no es correcta o el reconocedor no está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Este método realiza una operación de reconocimiento único. El reconocedor realiza esta operación en sus gramáticas de reconocimiento de voz cargados y habilitados.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
 El reconocedor no provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos cuando se usa este método.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> método devuelve un <xref:System.Speech.Recognition.RecognitionResult> objeto, o `null` si la operación no es correcta.  
  
 Una operación de reconocimiento sincrónica puede producir errores por las razones siguientes:  
  
-   No se detecta la voz antes de que caduquen los intervalos de tiempo de espera para la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propiedades.  
  
-   El motor de reconocimiento detecta la voz, pero no encuentra ninguna coincidencia en cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para llevar a cabo el reconocimiento asincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo se crea un <xref:System.Speech.Recognition.DictationGrammar>, lo carga en un módulo de reconocimiento de voz en curso y realiza una operación de reconocimiento.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Function Recognize (initialSilenceTimeout As TimeSpan) As RecognitionResult" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; System::Speech::Recognition::RecognitionResult ^ Recognize(TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="F#" Value="member this.Recognize : TimeSpan -&gt; System.Speech.Recognition.RecognitionResult" Usage="speechRecognitionEngine.Recognize initialSilenceTimeout" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">El intervalo de tiempo que un módulo de reconocimiento de voz acepta entrada que solo contiene silencio antes de finalizar el reconocimiento.</param>
        <summary>Realiza una operación de reconocimiento de voz sincrónica con un tiempo de espera de silencio inicial especificado.</summary>
        <returns>El resultado de reconocimiento para la entrada, o <see langword="null" /> si la operación no es correcta o el reconocedor no está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si el motor de reconocimiento de voz detecta la voz en el intervalo de tiempo especificado por `initialSilenceTimeout` argumento, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> realiza una operación de reconocimiento único y, a continuación, finaliza.  El `initialSilenceTimeout` parámetro reemplaza el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propiedad.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
 El reconocedor no provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos cuando se usa este método.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> método devuelve un <xref:System.Speech.Recognition.RecognitionResult> objeto, o `null` si la operación no es correcta.  
  
 Una operación de reconocimiento sincrónica puede producir errores por las razones siguientes:  
  
-   No se detecta la voz antes de que caduquen los intervalos de tiempo de espera para la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o para el `initialSilenceTimeout` parámetro.  
  
-   El motor de reconocimiento detecta la voz, pero no encuentra ninguna coincidencia en cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para llevar a cabo el reconocimiento asincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo se crea un <xref:System.Speech.Recognition.DictationGrammar>, lo carga en un módulo de reconocimiento de voz en curso y realiza una operación de reconocimiento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicia una operación de reconocimiento de voz asincrónica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Estos métodos realizan una o varias operaciones de reconocimiento asincrónica. El reconocedor realiza cada operación en sus gramáticas de reconocimiento de voz cargados y habilitados.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Se produce cuando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> termina la operación.  
  
 Para recuperar el resultado de una operación asincrónica de reconocimiento, adjuntar un controlador de eventos para el reconocimiento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. El reconocedor provoca este evento cada vez que complete correctamente una operación de reconocimiento sincrónicas o asincrónicas. Si no fue correcto, el reconocimiento de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propiedad <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que puede tener acceso en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Puede producir un error en una operación asincrónica de reconocimiento por las razones siguientes:  
  
-   No se detecta la voz antes de que caduquen los intervalos de tiempo de espera para la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propiedades.  
  
-   El motor de reconocimiento detecta la voz, pero no encuentra ninguna coincidencia en cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
-   El <xref:System.Speech.Recognition.SpeechRecognitionEngine> debe tener al menos un <xref:System.Speech.Recognition.Grammar> objeto cargado antes de realizar el reconocimiento. Para cargar una gramática de reconocimiento de voz, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
-   Para modificar cómo el reconocedor controla la temporización de voz o silencio con respecto al reconocimiento, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
-   Para llevar a cabo el reconocimiento de sincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Realiza una operación de reconocimiento de voz asincrónica única.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Este método realiza una operación de reconocimiento asincrónica, único. El reconocedor realiza la operación de sus gramáticas de reconocimiento de voz cargados y habilitados.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Se produce cuando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> termina la operación.  
  
 Para recuperar el resultado de una operación asincrónica de reconocimiento, adjuntar un controlador de eventos para el reconocimiento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. El reconocedor provoca este evento cada vez que complete correctamente una operación de reconocimiento sincrónicas o asincrónicas. Si no fue correcto, el reconocimiento de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propiedad <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que puede tener acceso en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Para llevar a cabo el reconocimiento de sincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz asincrónico básica. El ejemplo se crea un <xref:System.Speech.Recognition.DictationGrammar>, lo carga en un módulo de reconocimiento de voz en curso y realiza una operación asincrónica de reconocimiento. Controladores de eventos se incluyen para mostrar los eventos que provoca el reconocedor durante la operación.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsync (mode As RecognizeMode)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsync(System::Speech::Recognition::RecognizeMode mode);" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsync : System.Speech.Recognition.RecognizeMode -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsync mode" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Indica si se van realizar una o varias operaciones de reconocimiento.</param>
        <summary>Realiza una o más operaciones de reconocimiento de voz asincrónicas.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si `mode` es <xref:System.Speech.Recognition.RecognizeMode.Multiple>, el reconocedor continúa realizando operaciones de reconocimiento asincrónica hasta el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> se llama al método.  
  
 Durante una llamada a este método, el reconocedor puede provocar los eventos siguientes:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Se genera cuando el reconocedor detecta una entrada que puede identificar como voz.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Se genera cuando la entrada, crea una correspondencia ambigua con una de las gramáticas activas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Se genera cuando el reconocedor finaliza una operación de reconocimiento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Se produce cuando un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> termina la operación.  
  
 Para recuperar el resultado de una operación asincrónica de reconocimiento, adjuntar un controlador de eventos para el reconocimiento <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. El reconocedor provoca este evento cada vez que complete correctamente una operación de reconocimiento sincrónicas o asincrónicas. Si no fue correcto, el reconocimiento de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propiedad <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que puede tener acceso en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Puede producir un error en una operación asincrónica de reconocimiento por las razones siguientes:  
  
-   No se detecta la voz antes de que caduquen los intervalos de tiempo de espera para la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propiedades.  
  
-   El motor de reconocimiento detecta la voz, pero no encuentra ninguna coincidencia en cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para llevar a cabo el reconocimiento de sincrónico, utilice uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz asincrónico básica. El ejemplo se crea un <xref:System.Speech.Recognition.DictationGrammar>, lo carga en un módulo de reconocimiento de voz en curso y realiza varias operaciones de reconocimiento asincrónica. Después de 30 segundos, se cancelan las operaciones asincrónicas. Controladores de eventos se incluyen para mostrar los eventos que provoca el reconocedor durante la operación.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncCancel ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncCancel();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncCancel : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncCancel " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Finaliza el reconocimiento asincrónico sin esperar a que la operación actual de reconocimiento se complete.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Este método finaliza inmediatamente el reconocimiento asincrónico. Si la operación de reconocimiento asincrónica actual es recibir datos de entrada, la entrada se trunca y se completa la operación con la entrada existente. Reconocedor provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento cuando se cancela una operación asincrónica y establece el <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propiedad de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> a `true`. Este método cancela las operaciones asincrónicas iniciadas por el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Para detener el reconocimiento asincrónico sin truncar la entrada, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el uso de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> método. En el ejemplo se crea y carga una gramática de reconocimiento de voz, inicia una operación de reconocimiento asincrónica continua y, a continuación, detiene los 2 segundos antes de cancelar la operación. El reconocedor recibe una entrada del archivo, c:\temp\audioinput\sample.wav. Controladores de eventos se incluyen para mostrar los eventos que provoca el reconocedor durante la operación.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberSignature Language="VB.NET" Value="Public Sub RecognizeAsyncStop ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RecognizeAsyncStop();" />
      <MemberSignature Language="F#" Value="member this.RecognizeAsyncStop : unit -&gt; unit" Usage="speechRecognitionEngine.RecognizeAsyncStop " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Detiene el reconocimiento asincrónico después de que se complete la operación actual de reconocimiento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Este método finaliza el reconocimiento asincrónico sin truncamiento de entrada. Si la operación de reconocimiento asincrónica actual es recibir datos de entrada, el reconocedor continúa Aceptar la entrada hasta que se complete la operación actual de reconocimiento. Reconocedor provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento cuando se detiene una operación asincrónica y establece el <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propiedad de la <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> a `true`. Este método detiene las operaciones asincrónicas iniciadas por el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Para cancelar inmediatamente reconocimiento asincrónico con solo la entrada existente, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el uso de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> método. En el ejemplo se crea y carga una gramática de reconocimiento de voz, inicia una operación de reconocimiento asincrónica continua y, a continuación, detiene los 2 segundos antes de detenerse la operación. El reconocedor recibe una entrada del archivo, c:\temp\audioinput\sample.wav. Controladores de eventos se incluyen para mostrar los eventos que provoca el reconocedor durante la operación.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizeCompleted As EventHandler(Of RecognizeCompletedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizeCompletedEventArgs ^&gt; ^ RecognizeCompleted;" />
      <MemberSignature Language="F#" Value="member this.RecognizeCompleted : EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " Usage="member this.RecognizeCompleted : System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> concluye una operación de reconocimiento asincrónica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine> del objeto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método inicia una operación asincrónica de reconocimiento. Cuando el reconocedor finaliza la operación asincrónica, se genera este evento.  
  
 Con el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, puede tener acceso a la <xref:System.Speech.Recognition.RecognitionResult> en el <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto. Si no fue correcto, el reconocimiento de <xref:System.Speech.Recognition.RecognitionResult> será `null`. Para determinar si un tiempo de espera o una interrupción en la entrada de audio causa un error en el reconocimiento, puede obtener acceso a las propiedades de <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Para obtener más información, vea la clase <xref:System.Speech.Recognition.RecognizeCompletedEventArgs>.  
  
 Para obtener más detalles sobre las mejores candidatas reconocimiento rechazados, adjuntar un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventos.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente se reconoce frases como "Mostrar la lista de intérpretes de la categoría de jazz" o "Mostrar totalmente confiable álbumes". El ejemplo usa un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos para mostrar información sobre los resultados de reconocimiento en la consola.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizeCompletedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerAudioPosition As TimeSpan" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property TimeSpan RecognizerAudioPosition { TimeSpan get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerAudioPosition : TimeSpan" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene la ubicación actual de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> en la entrada de audio que está procesando.</summary>
        <value>La posición del reconocedor en la entrada de audio que está procesando.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 La posición de audio es específica para cada módulo de reconocimiento de voz. Cuando se habilita, se establece el valor cero de un flujo de entrada.  
  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> referencias de propiedad el <xref:System.Speech.Recognition.SpeechRecognitionEngine> la posición del objeto dentro de su entrada de audio. Por el contrario, el <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propiedad hace referencia a la posición del dispositivo de entrada en su secuencia de audio generado. Estas posiciones pueden ser diferentes. Por ejemplo, si el reconocedor ha recibido de entrada para que TI no tiene aún genera un resultado de reconocimiento, a continuación, el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propiedad es menor que el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propiedad.  
  
 ]]></format>
        </remarks>
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberSignature Language="VB.NET" Value="Public ReadOnly Property RecognizerInfo As RecognizerInfo" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; property System::Speech::Recognition::RecognizerInfo ^ RecognizerInfo { System::Speech::Recognition::RecognizerInfo ^ get(); };" />
      <MemberSignature Language="F#" Value="member this.RecognizerInfo : System.Speech.Recognition.RecognizerInfo" Usage="System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtiene información acerca de la instancia actual de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Información sobre el reconocedor de voz actual.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obtener información acerca de todos los reconocedores de voz instalado para el sistema actual, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente obtiene una lista parcial de los datos para el motor de reconocimiento de voz en el proceso actual. Para obtener más información, vea <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberSignature Language="VB.NET" Value="Public Event RecognizerUpdateReached As EventHandler(Of RecognizerUpdateReachedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::RecognizerUpdateReachedEventArgs ^&gt; ^ RecognizerUpdateReached;" />
      <MemberSignature Language="F#" Value="member this.RecognizerUpdateReached : EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " Usage="member this.RecognizerUpdateReached : System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando un objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> en ejecución hace una pausa para aceptar modificaciones.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Las aplicaciones deben usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar una instancia en ejecución de <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de modificar su configuración o su <xref:System.Speech.Recognition.Grammar> objetos. El <xref:System.Speech.Recognition.SpeechRecognitionEngine> provoca este evento cuando esté listo para aceptar modificaciones.  
  
 Por ejemplo, mientras el <xref:System.Speech.Recognition.SpeechRecognitionEngine> está en pausa, puede cargar, descargar, habilitar y deshabilitar <xref:System.Speech.Recognition.Grammar> objetos y modifique los valores de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propiedades. Para obtener más información, vea el método <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente muestra una aplicación de consola que carga y descarga <xref:System.Speech.Recognition.Grammar> objetos. La aplicación usa el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método para solicitar el motor de reconocimiento de voz para hacer una pausa de forma que pueda recibir una actualización. La aplicación, a continuación, carga o descarga un <xref:System.Speech.Recognition.Grammar> objeto.  
  
 En cada actualización, un controlador para <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos escribe su nombre y su estado cargado actualmente <xref:System.Speech.Recognition.Grammar> objetos en la consola. Como las gramáticas se cargan y descargan, la aplicación reconoce primero los nombres de los animales de granja de servidores, los nombres de los animales de granja de servidores y los nombres de frutas y, luego, solo los nombres de frutas.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.RecognizerUpdateReachedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Solicita que el reconocedor pause para actualizar su estado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Utilice este método para sincronizar los cambios al reconocedor. Por ejemplo, si carga o descarga una gramática de reconocimiento de voz mientras el reconocedor está procesando la entrada, utilice este método y el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos para sincronizar el comportamiento de su aplicación con el estado del reconocedor.  
  
 Cuando se llama a este método, el reconocedor pause o finaliza operaciones asincrónicas y genera un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos. Un <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> controlador de eventos, a continuación, puede modificar el estado del reconocedor entre operaciones de reconocimiento. Cuando se administran <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos, el reconocedor realiza una pausa hasta que devuelve el controlador de eventos.  
  
> [!NOTE]
>  Si se cambia la entrada al reconocedor antes reconocedor provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> se descarta el evento, la solicitud.  
  
 Cuando se llama a este método:  
  
-   Si el reconocedor no está procesando la entrada, el reconocedor genera inmediatamente la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos.  
  
-   Si el reconocedor procesa la entrada que consta de silencio o ruido de fondo, el reconocedor pause la operación de reconocimiento y genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos.  
  
-   Si el reconocedor procesa la entrada que no constan de silencio o ruido de fondo, el reconocedor finaliza la operación de reconocimiento y, a continuación, genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos.  
  
 Mientras que el reconocedor controla la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos:  
  
-   El reconocedor no procesa la entrada y el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propiedad sigue siendo el mismo.  
  
-   El reconocedor continúa para recopilar de entrada y el valor de la <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> puede cambiar la propiedad.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate();" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : unit -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Solicita que el reconocedor pause para actualizar su estado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cuando el reconocedor genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos, el <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propiedad de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> es `null`.  
  
 Para proporcionar un token de usuario, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método. Para especificar un desplazamiento de posición de audio, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra una aplicación de consola que carga y descarga <xref:System.Speech.Recognition.Grammar> objetos. La aplicación usa el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método para solicitar el motor de reconocimiento de voz para hacer una pausa de forma que pueda recibir una actualización. La aplicación, a continuación, carga o descarga un <xref:System.Speech.Recognition.Grammar> objeto.  
  
 En cada actualización, un controlador para <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos escribe su nombre y su estado cargado actualmente <xref:System.Speech.Recognition.Grammar> objetos en la consola. Como las gramáticas se cargan y descargan, la aplicación reconoce primero los nombres de los animales de granja de servidores, los nombres de los animales de granja de servidores y los nombres de frutas y, luego, solo los nombres de frutas.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate userToken" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Información definida por el usuario que contiene información para la operación.</param>
        <summary>Solicita que el reconocedor pause para actualizar su estado y proporcione un token de usuario para el evento asociado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cuando el reconocedor genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos, el <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propiedad de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene el valor de la `userToken` parámetro.  
  
 Para especificar un desplazamiento de posición de audio, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberSignature Language="VB.NET" Value="Public Sub RequestRecognizerUpdate (userToken As Object, audioPositionAheadToRaiseUpdate As TimeSpan)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void RequestRecognizerUpdate(System::Object ^ userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="F#" Value="member this.RequestRecognizerUpdate : obj * TimeSpan -&gt; unit" Usage="speechRecognitionEngine.RequestRecognizerUpdate (userToken, audioPositionAheadToRaiseUpdate)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Información definida por el usuario que contiene información para la operación.</param>
        <param name="audioPositionAheadToRaiseUpdate">El desplazamiento del <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> actual para retrasar la solicitud.</param>
        <summary>Solicita que el reconocedor pause para actualizar su estado y proporcione un desplazamiento y un token de usuario para el evento asociado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor no inicia la solicitud de actualización del reconocedor hasta que el reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> es igual a la actual <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> plus `audioPositionAheadToRaiseUpdate`.  
  
 Cuando el reconocedor genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos, el <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propiedad de la <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contiene el valor de la `userToken` parámetro.  
  
 ]]></format>
        </remarks>
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
        <altmember cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToAudioStream (audioSource As Stream, audioFormat As SpeechAudioFormatInfo)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToAudioStream(System::IO::Stream ^ audioSource, System::Speech::AudioFormat::SpeechAudioFormatInfo ^ audioFormat);" />
      <MemberSignature Language="F#" Value="member this.SetInputToAudioStream : System.IO.Stream * System.Speech.AudioFormat.SpeechAudioFormatInfo -&gt; unit" Usage="speechRecognitionEngine.SetInputToAudioStream (audioSource, audioFormat)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">Flujo de entrada de audio.</param>
        <param name="audioFormat">El formato de la entrada de audio.</param>
        <summary>Configura el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para recibir entradas de una secuencia de audio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si el reconocedor llega al final del flujo de entrada durante una operación de reconocimiento, la operación de reconocimiento finaliza con la entrada disponible. Las operaciones posteriores de reconocimiento pueden generar una excepción, a menos que actualice la entrada para el reconocimiento.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. El ejemplo usa la entrada de un archivo de audio, example.wav, que contiene las frases, "prueba comprobación uno dos tres" y "señor cooper", separados por una pausa. El ejemplo genera el siguiente resultado.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToDefaultAudioDevice ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToDefaultAudioDevice();" />
      <MemberSignature Language="F#" Value="member this.SetInputToDefaultAudioDevice : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToDefaultAudioDevice " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Configura el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para recibir entradas del dispositivo de audio predeterminado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra el reconocimiento de voz básica. En el ejemplo se utiliza la salida desde el dispositivo de audio de forma predeterminada, realiza varias operaciones de reconocimiento asincrónica y se cierra cuando un usuario utters la frase "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToNull ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToNull();" />
      <MemberSignature Language="F#" Value="member this.SetInputToNull : unit -&gt; unit" Usage="speechRecognitionEngine.SetInputToNull " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Deshabilita la entrada al módulo de reconocimiento de voz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurar la <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto para ninguna entrada cuando se usa el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos, o cuando se toma un motor de reconocimiento temporalmente sin conexión.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveFile (path As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveFile(System::String ^ path);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveFile : string -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveFile path" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">Ruta de acceso del archivo que se va a usar como entrada.</param>
        <summary>Configura el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para recibir entradas de un archivo de formato de audio de forma de onda (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si el reconocedor llega al final del archivo de entrada durante una operación de reconocimiento, la operación de reconocimiento finaliza con la entrada disponible. Las operaciones posteriores de reconocimiento pueden generar una excepción, a menos que actualice la entrada para el reconocimiento.  
  
   
  
## Examples  
 El ejemplo siguiente realiza el reconocimiento en el audio en un archivo .wav y escribe el texto reconocido en la consola.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberSignature Language="VB.NET" Value="Public Sub SetInputToWaveStream (audioSource As Stream)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void SetInputToWaveStream(System::IO::Stream ^ audioSource);" />
      <MemberSignature Language="F#" Value="member this.SetInputToWaveStream : System.IO.Stream -&gt; unit" Usage="speechRecognitionEngine.SetInputToWaveStream audioSource" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">Secuencia que contiene los datos de audio.</param>
        <summary>Configura el objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para recibir entradas de un flujo que contiene datos de formato de audio de forma de onda (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si el reconocedor llega al final del flujo de entrada durante una operación de reconocimiento, la operación de reconocimiento finaliza con la entrada disponible. Las operaciones posteriores de reconocimiento pueden generar una excepción, a menos que actualice la entrada para el reconocimiento.  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechDetected As EventHandler(Of SpeechDetectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechDetectedEventArgs ^&gt; ^ SpeechDetected;" />
      <MemberSignature Language="F#" Value="member this.SpeechDetected : EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " Usage="member this.SpeechDetected : System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> detecta una entrada que puede identificar como voz.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada módulo de reconocimiento de voz tiene un algoritmo para distinguir entre latencia y voz. Cuando el <xref:System.Speech.Recognition.SpeechRecognitionEngine> realiza una operación de reconocimiento de voz, genera el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento cuando su algoritmo identifica la entrada como voz. El <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> propiedad del asociado <xref:System.Speech.Recognition.SpeechDetectedEventArgs> objeto indica la ubicación en la secuencia de entrada que el reconocedor detecta la voz. El <xref:System.Speech.Recognition.SpeechRecognitionEngine> provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento antes de que produce cualquiera de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventos.  
  
 Para obtener más información, consulte el <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente es parte de una aplicación de consola para elegir las ciudades de origen y destino de un vuelo. La aplicación reconoce frases como "Quiero Volar de Miami en Chicago".  El ejemplo se usa el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> eventos al informe el <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> se detecta la voz de cada hora.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberSignature Language="VB.NET" Value="Public Custom Event SpeechHypothesized As EventHandler(Of SpeechHypothesizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechHypothesizedEventArgs ^&gt; ^ SpeechHypothesized;" />
      <MemberSignature Language="F#" Value="member this.SpeechHypothesized : EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " Usage="member this.SpeechHypothesized : System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ha reconocido una palabra o palabras que pueden ser un componente de varias frases completas en una gramática.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El <xref:System.Speech.Recognition.SpeechRecognitionEngine> genera numerosas <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventos tal como se trata de identificar una frase de entrada. Se puede tener acceso al texto de frases reconocidas parcialmente en el <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propiedad de la <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> objeto en el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventos. Normalmente, estos eventos de control es útil sólo para la depuración.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> se deriva de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Para obtener más información, consulte el <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedad y el <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente reconoce frases como "Mostrar la lista de intérpretes en la categoría de jazz". El ejemplo se usa el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventos para mostrar fragmentos frases incompletas en la consola, tal como se reconocen.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognitionRejected As EventHandler(Of SpeechRecognitionRejectedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognitionRejectedEventArgs ^&gt; ^ SpeechRecognitionRejected;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognitionRejected : EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " Usage="member this.SpeechRecognitionRejected : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> recibe una entrada que no coincide con ninguno de los objetos <see cref="T:System.Speech.Recognition.Grammar" /> cargados y habilitados.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 El reconocedor provoca este evento si determina que entrada no coincide con la confianza suficiente cualquiera de sus cargados y habilitados <xref:System.Speech.Recognition.Grammar> objetos. El <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propiedad de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene el rechazadas <xref:System.Speech.Recognition.RecognitionResult> objeto. Puede usar el controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> event para recuperar el reconocimiento de <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> que se han rechazado y sus <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> puntuaciones.  
  
 Si la aplicación utiliza un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia, puede modificar el nivel de confianza en qué voz entrada se acepta o rechaza con uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos. Puede modificar cómo responde el reconocimiento de voz no voz de entrada mediante el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente se reconoce frases como "Mostrar la lista de intérpretes de la categoría de jazz" o "Mostrar totalmente confiable álbumes". El ejemplo usa un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventos para mostrar una notificación en la consola cuando el contenido de voz de entrada no coincide con el contenido de la gramática con suficiente <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> para generar un reconocimiento correcto. El controlador también muestra el resultado de reconocimiento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> que se han rechazado debido a las puntuaciones de confianza baja.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberSignature Language="VB.NET" Value="Public Event SpeechRecognized As EventHandler(Of SpeechRecognizedEventArgs) " />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; event EventHandler&lt;System::Speech::Recognition::SpeechRecognizedEventArgs ^&gt; ^ SpeechRecognized;" />
      <MemberSignature Language="F#" Value="member this.SpeechRecognized : EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " Usage="member this.SpeechRecognized : System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; " />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Se genera cuando <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> recibe una entrada que coincide con cualquiera de los objetos <see cref="T:System.Speech.Recognition.Grammar" /> cargados y habilitados.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Puede iniciar una operación de reconocimiento mediante uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos. Reconocedor provoca la <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> si determina que la entrada coincide con uno de su carga de evento <xref:System.Speech.Recognition.Grammar> objetos con un nivel suficiente de confianza para constituir el reconocimiento. El <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propiedad de la <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contiene aceptado <xref:System.Speech.Recognition.RecognitionResult> objeto. Controladores de <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos pueden obtener la frase reconocida, así como una lista de reconocimiento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> con las puntuaciones de confianza inferior.  
  
 Si la aplicación utiliza un <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia, puede modificar el nivel de confianza en qué voz entrada se acepta o rechaza con uno de los <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos.  Puede modificar cómo responde el reconocimiento de voz no voz de entrada mediante el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
 Cuando el reconocedor recibe una entrada que coincide con una gramática, la <xref:System.Speech.Recognition.Grammar> objeto puede elevar su <xref:System.Speech.Recognition.Grammar.SpeechRecognized> eventos. El <xref:System.Speech.Recognition.Grammar> del objeto <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento se produce antes que el reconocedor de voz <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. Todas las tareas específicas con una gramática determinada siempre deben realizarse mediante un controlador para el <xref:System.Speech.Recognition.Grammar.SpeechRecognized> eventos.  
  
 Cuando se crea un delegado <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, se identifica el método que controlará el evento. Para asociar el evento al controlador, se debe agregar una instancia del delegado al evento. Siempre que se produce el evento, se llama a su controlador, a menos que se quite el delegado. Para obtener más información sobre los delegados de controlador de eventos, consulte [eventos y delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 El ejemplo siguiente es parte de una aplicación de consola que crea la gramática de reconocimiento de voz, construcciones un <xref:System.Speech.Recognition.Grammar> objeto y los carga en el <xref:System.Speech.Recognition.SpeechRecognitionEngine> para realizar el reconocimiento. El ejemplo muestra la entrada de voz a una <xref:System.Speech.Recognition.SpeechRecognitionEngine>, los resultados de reconocimiento asociados y los eventos asociados generados por el reconocedor de voz.  
  
 Se habla de entrada, como "Quiero Volar de Chicago a Miami" desencadenará una <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. La frase "Volar me de Houston a Chicago" en términos no desencadenará una <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos.  
  
 El ejemplo usa un controlador para el <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> reconoce el evento para mostrar correctamente las frases y la semántica que contienen en la consola.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="T:System.Speech.Recognition.SpeechDetectedEventArgs" />
        <altmember cref="T:System.Speech.Recognition.Grammar" />
        <altmember cref="T:System.Speech.Recognition.RecognitionResult" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
        <altmember cref="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
        <altmember cref="T:System.Speech.Recognition.RecognitionEventArgs" />
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberSignature Language="VB.NET" Value="Public Sub UnloadAllGrammars ()" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadAllGrammars();" />
      <MemberSignature Language="F#" Value="member this.UnloadAllGrammars : unit -&gt; unit" Usage="speechRecognitionEngine.UnloadAllGrammars " />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Descarga todos los objetos <see cref="T:System.Speech.Recognition.Grammar" /> del reconocedor.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si el reconocedor está cargando un <xref:System.Speech.Recognition.Grammar> de forma asincrónica, este método espera hasta que el <xref:System.Speech.Recognition.Grammar> se cargan antes de descarga todos el <xref:System.Speech.Recognition.Grammar> objetos desde el <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia.  
  
 Para descargar una gramática específica, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra la carga sincrónica y descarga de las gramáticas reconocimiento de voz.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UnloadGrammar(System::Speech::Recognition::Grammar ^ grammar);" />
      <MemberSignature Language="F#" Value="member this.UnloadGrammar : System.Speech.Recognition.Grammar -&gt; unit" Usage="speechRecognitionEngine.UnloadGrammar grammar" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">El objeto de gramática para descargar.</param>
        <summary>Descarga un objeto <see cref="T:System.Speech.Recognition.Grammar" /> especificado de la instancia de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Si se está ejecutando el reconocedor, las aplicaciones deben usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar la <xref:System.Speech.Recognition.SpeechRecognitionEngine> instancia antes de cargar, descargar, habilitar o deshabilitar un <xref:System.Speech.Recognition.Grammar> objeto. Descargar todo <xref:System.Speech.Recognition.Grammar> objetos, utilice el <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> método.  
  
   
  
## Examples  
 El ejemplo siguiente muestra parte de una aplicación de consola que muestra la carga sincrónica y descarga de las gramáticas reconocimiento de voz.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> es <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">La gramática no se carga en este reconocedor, o este reconocedor está cargando actualmente la gramática de forma asincrónica.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Actualiza el valor de una configuración para el reconocedor.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configuración del módulo de reconocimiento puede contener la cadena, entero de 64 bits o datos de la dirección de memoria. En la tabla siguiente se describe la configuración que se define para una API de Microsoft Speech (SAPI)-reconocedor compatible. La siguiente configuración debe tener el mismo intervalo para cada módulo de reconocimiento que admita la configuración. Un reconocedor conforme SAPI no es necesario para admitir esta configuración y puede admitir otras opciones.  
  
|nombre|Descripción|  
|----------|-----------------|  
|`ResourceUsage`|Especifica el consumo de CPU del reconocedor. El intervalo es de 0 a 100. El valor predeterminado es 50.|  
|`ResponseSpeed`|Indica la longitud de silencio al final de la entrada no ambigua antes de que el reconocedor de voz completa una operación de reconocimiento. El intervalo es de 0 a 10 000 milisegundos (ms). Esta opción corresponde al reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propiedad. Valor predeterminado = 150 ms.|  
|`ComplexResponseSpeed`|Indica la longitud de latencia en milisegundos (ms) al final de la entrada ambigua antes de que el reconocedor de voz completa una operación de reconocimiento. El intervalo es de 0 a 10 000 ms. Esta opción corresponde al reconocedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedad. Valor predeterminado = 500 ms.|  
|`AdaptationOn`|Indica si la adaptación del modelo acústico es ON (valor = `1`) u OFF (valor = `0`). El valor predeterminado es `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica si la adaptación de fondo es ON (valor = `1`) u OFF (valor = `0`), y se conserva la configuración en el registro. El valor predeterminado es `1` (ON).|  
  
 Para devolver uno de los valores del reconocedor, use el <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> método.  
  
 Con la excepción de `PersistedBackgroundAdaptation`, los valores de propiedad establecidos mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos siguen en vigor solo para la instancia actual de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, después de que revierta a sus valores predeterminados.  
  
 Puede modificar cómo responde el reconocimiento de voz no voz de entrada mediante el <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, y <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propiedades.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As Integer)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, int updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * int -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">Nombre del la configuración que va a actualizarse.</param>
        <param name="updatedValue">Nuevo valor de la configuración.</param>
        <summary>Actualiza el valor especificado para <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> con el valor entero especificado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Con la excepción de `PersistedBackgroundAdaptation`, los valores de propiedad establecidos mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> método siguen en vigor solo para la instancia actual de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, después de que revierta a sus valores predeterminados. Consulte <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> para obtener descripciones de las opciones admitidas.  
  
   
  
## Examples  
 El ejemplo siguiente es parte de una aplicación de consola que genera los valores de un número de la configuración definida para el reconocedor que admita la configuración regional en-US. En el ejemplo se actualiza la configuración de nivel de confianza y, a continuación, consulta el reconocedor para comprobar los valores actualizados. El ejemplo genera el siguiente resultado.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> es la cadena vacía ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">El reconocedor no tiene un valor con ese nombre.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberSignature Language="VB.NET" Value="Public Sub UpdateRecognizerSetting (settingName As String, updatedValue As String)" />
      <MemberSignature Language="C++ CLI" Value="public:&#xA; void UpdateRecognizerSetting(System::String ^ settingName, System::String ^ updatedValue);" />
      <MemberSignature Language="F#" Value="member this.UpdateRecognizerSetting : string * string -&gt; unit" Usage="speechRecognitionEngine.UpdateRecognizerSetting (settingName, updatedValue)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>3.0.0.0</AssemblyVersion>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">Nombre del la configuración que va a actualizarse.</param>
        <param name="updatedValue">Nuevo valor de la configuración.</param>
        <summary>Actualiza el valor especificado del motor de reconocimiento de voz con el valor de cadena especificado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Con la excepción de `PersistedBackgroundAdaptation`, los valores de propiedad establecidos mediante la <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> método siguen en vigor solo para la instancia actual de <xref:System.Speech.Recognition.SpeechRecognitionEngine>, después de que revierta a sus valores predeterminados. Consulte <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> para obtener descripciones de las opciones admitidas.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> es <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> es la cadena vacía ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">El reconocedor no tiene un valor con ese nombre.</exception>
        <altmember cref="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      </Docs>
    </Member>
  </Members>
</Type>